{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1kUc09gXR3X4HRsWm_iv3XiaVkIBUY4Dq",
     "timestamp": 1678390093267
    }
   ]
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "id": "crEXE4J9Cu6M"
   },
   "source": [
    "from abc import ABC, abstractmethod\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "OL4bP-lxCxy5"
   },
   "source": [
    "file = open(\"dane/dane{}.txt\".format(1), \"r\")\n",
    "X = []\n",
    "y = []\n",
    "for x in file:\n",
    "  point = x.split(\" \")\n",
    "  point.remove('\\n')\n",
    "  X.append(float(point[0]))\n",
    "  y.append(float(point[1]))\n",
    "\n",
    "#norm=np.linalg.norm(y) #opcjonalnie\n",
    "#y/=norm #opcjonalnie\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class ActivationFunction(ABC):\n",
    "  @abstractmethod\n",
    "  def derivative(self, x):\n",
    "    pass\n",
    "  @abstractmethod\n",
    "  def __call__(self, x):\n",
    "    pass"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "XnJLbhwTMXiB"
   },
   "source": [
    "class Sigmoid(ActivationFunction):\n",
    "  def __call__(self, x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "  def derivative(self, x):\n",
    "    a = self(x)\n",
    "    return a * (1 - a)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "kjqGu_iHRJwK"
   },
   "source": [
    "class Relu(ActivationFunction):\n",
    "  def __call__(self, x):\n",
    "    return np.max(0, x)\n",
    "  def derivative(self, x):\n",
    "    Sigmoid()(x)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "yndTYtmVDCKM"
   },
   "source": [
    "class Neuron:\n",
    "  def __init__(self, input: int, f: ActivationFunction, eta: float):\n",
    "    self.W = np.random.rand(input)\n",
    "    self.Wb = np.random.rand(1)[0]\n",
    "    self.f = f\n",
    "    self.eta = eta\n",
    "    self.s = 0\n",
    "\n",
    "  def predict(self, x):\n",
    "    self.s = np.dot(x, self.W) + self.Wb\n",
    "    y = self.f(self.s)\n",
    "    return y, self.s\n",
    "\n",
    "  def fit(self, e, s):\n",
    "    d = self.f.derivative(s) * e\n",
    "    self.W += self.eta * d * self.W * e\n",
    "    return 1."
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "l4to4uwwDJCH"
   },
   "source": [
    "# class Layer:\n",
    "#   def __init__(self, input, output, f, eta):\n",
    "#     self.neurons = []\n",
    "#     for i in range(output):\n",
    "#       self.neurons.append(Neuron(input, f, eta))\n",
    "#\n",
    "#   def predict(self, x):\n",
    "#     output = np.zeros(len(self.neurons))\n",
    "#     s = []\n",
    "#     for i, n in enumerate(self.neurons):\n",
    "#       output[i], s = n.predict(x)\n",
    "#     return output, s\n",
    "#\n",
    "#   def fit(self, e, s):\n",
    "#     #Zapisac wagi dla wyliczenia bledu do poprzedniej warstwy\n",
    "#     #Wykonac korekcje wag dla aktualnej warstwy\n",
    "#     return 1.\n",
    "#\n",
    "#   def getW(self):\n",
    "#     allW = []\n",
    "#     for n in self.neurons:\n",
    "#       for w in n.W:\n",
    "#         allW.append(w)\n",
    "#     return np.array(allW)\n",
    "\n",
    "class Layer:\n",
    "  def __init__(self, input, output, f: ActivationFunction, eta):\n",
    "    self.weights = np.random.rand(input, output)\n",
    "    self.bias = np.random.rand(input)\n",
    "    self.f = f\n",
    "    self.alpha = eta\n",
    "\n",
    "  def predict(self, x):\n",
    "    self.z = self.weights @ x + self.bias\n",
    "    self.a = self.f(self.z)\n",
    "    return self.a\n",
    "\n",
    "  def fit(self, e, s):\n",
    "    #Zapisac wagi dla wyliczenia bledu do poprzedniej warstwy\n",
    "    #Wykonac korekcje wag dla aktualnej warstwy\n",
    "    return 1.\n",
    "\n",
    "  def getW(self):\n",
    "    allW = []\n",
    "    for n in self.neurons:\n",
    "      for w in n.W:\n",
    "        allW.append(w)\n",
    "    return np.array(allW)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Rp6JowXcDJhh"
   },
   "source": [
    "# class NeuronNetwork:\n",
    "#   def __init__(self, layers, acti, eta):\n",
    "#     self.layers=[]\n",
    "#     for i in range(1, len(layers)):\n",
    "#       self.layers.append(Layer(layers[i-1], layers[i], acti, eta))\n",
    "#\n",
    "#   def predict(self, x):\n",
    "#     s = np.array([])\n",
    "#     out = x\n",
    "#     for layer in self.layers:\n",
    "#       out, si = layer.predict(out)\n",
    "#       s.append(si)\n",
    "#     return out, s\n",
    "#\n",
    "#   def fit(self, X, y, e, s):\n",
    "#     #Idac od warstwy wyjsciowej, pobrac wagi i blad aktualnej warstwy do wyliczenia bledu dla kolejnej, i przeprowadzic trening aktualnej warstwy.\n",
    "#     return 1.\n",
    "\n",
    "class NeuronNetwork:\n",
    "  def __init__(self, layers, acti, eta):\n",
    "    self.layers = []\n",
    "    for i in range(1, len(layers)):\n",
    "      self.layers.append(Layer(layers[i-1], layers[i], acti, eta))\n",
    "\n",
    "  def predict(self, x):\n",
    "    out = x\n",
    "    for layer in self.layers:\n",
    "      out = layer.predict(out)\n",
    "    return out\n",
    "\n",
    "  def backpropagate(self, y):\n",
    "    errors = np.zeros(len(self.layers))\n",
    "    partials = np.zeros((self.layers-1, 2))\n",
    "    last = self.layers[-1]\n",
    "    errors[-1] = -(y - last.a) * last.f.derivative(last.z)\n",
    "    for i, layer in reversed(enumerate(self.layers[:-1])):\n",
    "      errors[i] = layer.weights.T * errors[i+1] * layer.f.derivative(layer.z)\n",
    "      partials[i, 0], partials[i, 1] = errors[i+1] @ layer.a.T, errors[i+1]\n",
    "    return errors, partials\n",
    "\n",
    "  def fit(self, X, y, e, s):\n",
    "    for\n",
    "    #Idac od warstwy wyjsciowej, pobrac wagi i blad aktualnej warstwy do wyliczenia bledu dla kolejnej, i przeprowadzic trening aktualnej warstwy.\n",
    "    return 1."
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "AkdJM0-vHlIG"
   },
   "source": [
    "def online(NN, epoch=100):\n",
    "  for i in range(epoch):\n",
    "    for x, y in zip(X_train, y_train):\n",
    "      e, s = NN.predict(x)\n",
    "      e -= y\n",
    "      NN.fit(X_train, y_train, e, s)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "h4XmgsdlHoLc"
   },
   "source": [
    "def batch(NN, epoch=100):\n",
    "  for i in range(epoch):\n",
    "    e = 0\n",
    "    s = 0\n",
    "    for x, y in zip(X_train, y_train):\n",
    "      en, sn = NN.predict(x)\n",
    "      e += (en-y)\n",
    "      s += sn\n",
    "    e /= len(X_train)\n",
    "    s /= len(X_train)\n",
    "    NN.fit(X_train, y_train, e, s)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "hKXLH2tKI_a6"
   },
   "source": [
    "batch(NeuronNetwork([1, 10, 20, 50, 1], Sigmoid(), 0.001), 1)\n",
    "online(NeuronNetwork([1, 10, 20, 50, 1], Sigmoid(), 0.001), 1)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "eKhGdULkbWP6"
   },
   "source": [
    "#utworzyc model, wytrenowac i przetestowac\n",
    "#pomanipulowac iloscia neuronow, warstw, funkcji aktywacji, wspolczynikiem uczenia i iloscia epok"
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}
